{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 3.4: Bloc de notas del estudiante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Información general\n",
    "\n",
    "Este laboratorio es una continuación de los laboratorios guiados del Módulo 3. \n",
    "\n",
    "En este laboratorio, dividirá los datos en tres conjuntos de datos separados:\n",
    "\n",
    "- *Conjunto de entrenamiento*: se utilizará para entrenar el modelo.\n",
    "- *Conjunto de validación*: se utilizará durante el entrenamiento para validar el modelo.\n",
    "- *Conjunto de prueba*: este se retendrá y se utilizará para producir métricas después de entrenar el modelo. Utilizará este conjunto de datos en un próximo laboratorio.\n",
    "\n",
    "Con los datos divididos, entrenará un modelo XGBoost utilizando Amazon SageMaker.\n",
    "\n",
    "\n",
    "## Introducción al escenario empresarial\n",
    "\n",
    "Usted trabaja para un proveedor de atención médica y desea mejorar la detección de anomalías en pacientes ortopédicos. \n",
    "\n",
    "Tiene la tarea de resolver este problema mediante el aprendizaje automático. Tiene acceso a un conjunto de datos que contiene seis características biomecánicas y un objetivo de *normal* o *anormal*. Puede utilizar este conjunto de datos para entrenar un modelo de aprendizaje automático para predecir si un paciente va a tener una anomalía.\n",
    "\n",
    "\n",
    "## Acerca de este conjunto de datos\n",
    "\n",
    "Este conjunto de datos biomédicos fue creado por el Dr. Henrique da Mota durante un período de residencia médica en el Grupo de Investigación Aplicada en Ortopedia (GARO, Group of Applied Research in Orthopaedics) del Centre Médico-Chiurgical de Réadaptation des Massues, Lyon, Francia. Los datos se han organizado en dos tareas de clasificación diferentes, pero relacionadas. \n",
    "\n",
    "La primera tarea consiste en clasificar a los pacientes como pertenecientes a una de tres categorías: \n",
    "\n",
    "- *Normal* (100 pacientes)\n",
    "- *Hernia de disco* (60 pacientes)\n",
    "- *Espondilolistesis* (150 pacientes)\n",
    "\n",
    "Para la segunda tarea, las categorías *Hernia de disco* y *Espondilolistesis* se fusionaron en una sola categoría etiquetada como *anormal*. Por lo tanto, la segunda tarea consiste en clasificar a los pacientes como pertenecientes a una de dos categorías: *Normal* (100 pacientes) o *Anormal* (210 pacientes).\n",
    "\n",
    "\n",
    "## Información de atributos:\n",
    "\n",
    "Cada paciente está representado en el conjunto de datos por seis atributos biomecánicos que se derivan de la forma y orientación de la pelvis y la columna lumbar (en este orden): \n",
    "\n",
    "- Incidencia pélvica\n",
    "- Inclinación pélvica\n",
    "- Ángulo de lordosis lumbar\n",
    "- Inclinación del sacro\n",
    "- Radio pélvico\n",
    "- Grado de espondilolistesis\n",
    "\n",
    "La siguiente convención se utiliza para las etiquetas de clase: \n",
    "- Hernia Disco (HD)\n",
    "- Espondilolistesis (EL)\n",
    "- Normal (NO) \n",
    "- Anormal (AN)\n",
    "\n",
    "\n",
    "Para obtener más información acerca de este conjunto de datos, consulte [Página web del conjunto de datos de la columna vertebral] (http://archive.ics.uci.edu/ml/datasets/Vertebral+Column).\n",
    "\n",
    "\n",
    "## Atribuciones del conjunto de datos\n",
    "\n",
    "Este conjunto de datos se obtuvo de:\n",
    "Dua, D. y Graff, C. (2019). Repositorio de aprendizaje automático de la UCI (http://archive.ics.uci.edu/ml). Irvine, CA: Universidad de California, Escuela de Ciencias de la Información e Informática.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración del laboratorio\n",
    "Dado que esta solución se divide en varios laboratorios del módulo, debe ejecutar las siguientes celdas para poder cargar los datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de los datos\n",
    "\n",
    "Al ejecutar las celdas siguientes, los datos se importarán y estarán listos para su uso. \n",
    "\n",
    "**Nota:** Las siguientes celdas representan los pasos clave en los laboratorios anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, requests, zipfile, io\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_zip = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00212/vertebral_column_data.zip'\n",
    "r = requests.get(f_zip, stream=True)\n",
    "Vertebral_zip = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "Vertebral_zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arff.loadarff('column_2C_weka.arff')\n",
    "df = pd.DataFrame(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapper = {b'Abnormal':1,b'Normal':0}\n",
    "df['class']=df['class'].replace(class_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1: Explorar los datos\n",
    "Comenzará con un recordatorio rápido de los datos del conjunto de datos.\n",
    "\n",
    "Para sacar el máximo provecho de este laboratorio, lea detenidamente las instrucciones y el código antes de ejecutar las celdas. ¡Tómese el tiempo para experimentar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, use **forma** para examinar el número de filas y columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, obtenga una lista de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle',\n",
       "       'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver las seis características biomecánicas y que la columna objetivo se denomina *clase*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2: Preparar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este laboratorio, debe dividir los datos en tres conjuntos de datos.\n",
    "\n",
    "Una búsqueda en Internet mostrará muchas maneras diferentes de dividir conjuntos de datos. Muchos ejemplos de código que puede encontrar dividirán el conjunto de datos en *objetivo* y *características*. Luego, dividirán cada uno de esos dos conjuntos de datos en tres subconjuntos, lo que da como resultado un total de seis conjuntos de datos para realizar un seguimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traslado de la posición de la columna objetivo\n",
    "\n",
    "XGBoost requiere que los datos de entrenamiento estén en un solo archivo. El archivo debe tener el valor objetivo como la primera columna. \n",
    "\n",
    "Obtenga la columna objetivo y muévala a la primera posición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debería ver que la **clase** es ahora la primera columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class', 'pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle',\n",
       "       'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de los datos\n",
    "\n",
    "Comenzará dividiendo el conjunto de datos en dos conjuntos de datos. Utilizará un conjunto de datos para el entrenamiento y volverá a dividir el otro conjunto de datos para utilizarlo con la validación y las pruebas.\n",
    "\n",
    "Utilizará la función *división_entrenamiento_prueba* de la *biblioteca scikit-learn*, que es una biblioteca de aprendizaje automático gratuita para Python. Tiene muchos algoritmos y funciones útiles, como la que va a utilizar. \n",
    "\n",
    "- Para obtener más información acerca de la función, consulte la [Documentación de división de entrenamiento_prueba](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). \n",
    " - Para obtener más información sobre scikit-learn, consulte la [guía scikit-learn] (https://scikit-learn.org/stable/)\n",
    "\n",
    "Debido a que no tiene muchos datos, desea asegurarse de que los conjuntos de datos divididos contienen una cantidad representativa de cada clase. Por lo tanto, deberá usar el modificador *estratificar*. Finalmente, usará un número aleatorio para poder repetir las divisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test_and_validate = train_test_split(df, test_size=0.2, random_state=42, stratify=df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, divida el conjunto de datos *probar_y_validar* en dos partes iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, validate = train_test_split(test_and_validate, test_size=0.5, random_state=42, stratify=test_and_validate['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine los tres conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248, 7)\n",
      "(31, 7)\n",
      "(31, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, compruebe la distribución de las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    168\n",
      "0     80\n",
      "Name: class, dtype: int64\n",
      "1    21\n",
      "0    10\n",
      "Name: class, dtype: int64\n",
      "1    21\n",
      "0    10\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['class'].value_counts())\n",
    "print(test['class'].value_counts())\n",
    "print(validate['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos a Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost cargará los datos para el entrenamiento de Amazon Simple Storage Service (Amazon S3). Por lo tanto, debe escribir los datos en un archivo de valores separados por comas (CSV) y luego cargar el archivo en Amazon S3.\n",
    "\n",
    "Comience configurando algunas variables en el bucket de S3 y, a continuación, cree una función para cargar el archivo CSV en Amazon S3. Puede volver a usar esta función.\n",
    "\n",
    "Primero, explore la función.\n",
    "\n",
    "Observe la siguiente línea:\n",
    "\n",
    "`dataframe.to_csv(csv_buffer, header=False, index=False)`\n",
    "\n",
    "Esta línea escribe el DataFrame de pandas (que se pasó a la función) en el búfer de E/S, que se denomina *búfer_csv*. Usted utiliza un búfer dado que no es necesario escribir el archivo en forma local.\n",
    "\n",
    "Para evitar que se escriban los encabezados de las columnas, utilice `header=False`. Para evitar que se produzca la salida del índice de pandas, use `index=False`.\n",
    "\n",
    "Para escribir el búfer_csv en Amazon S3 como un objeto, utilice la operación `put` en `object`, que es una propiedad de `bucket`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='c47992a689653l4129474t1w374228752902-labbucket-17il0lfebcov2'\n",
    "\n",
    "prefix='lab3'\n",
    "\n",
    "train_file='vertebral_train.csv'\n",
    "test_file='vertebral_test.csv'\n",
    "validate_file='vertebral_validate.csv'\n",
    "\n",
    "import os\n",
    "\n",
    "s3_resource = boto3.Session().resource('s3')\n",
    "def upload_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False)\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilice la función que creó para cargar los tres conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_s3_csv(train_file, 'train', train)\n",
    "upload_s3_csv(test_file, 'test', test)\n",
    "upload_s3_csv(validate_file, 'validate', validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3: Entrenar el modelo\n",
    "\n",
    "Ahora que los datos están en Amazon S3, puede entrenar un modelo. \n",
    "\n",
    "El primer paso es obtener el URI del contenedor XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.image_uris import retrieve\n",
    "container = retrieve('xgboost',boto3.Session().region_name,'1.0-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, debe establecer algunos *hiperparámetros* para el modelo. Dado que es la primera vez que está entrenando el modelo, puede usar algunos valores para comenzar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams={\"num_round\":\"42\",\n",
    "             \"eval_metric\": \"auc\",\n",
    "             \"objective\": \"binary:logistic\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilice la función **estimador** para configurar el modelo. Aquí hay algunos parámetros de interés:\n",
    "\n",
    "- **recuento_de_instancia**: define cuántas instancias se utilizarán para el entrenamiento. Utilizará *una* instancia.\n",
    "- **tipo_de_instancia**: define el tipo de instancia para el entrenamiento. En este caso, es *ml.m4.xlarge*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)\n",
    "xgb_model=sagemaker.estimator.Estimator(container,\n",
    "                                       sagemaker.get_execution_role(),\n",
    "                                       instance_count=1,\n",
    "                                       instance_type='ml.m4.xlarge',\n",
    "                                       output_path=s3_output_location,\n",
    "                                        hyperparameters=hyperparams,\n",
    "                                        sagemaker_session=sagemaker.Session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estimador necesita *canales* para ingresar datos en el modelo. Para el entrenamiento, se usarán el *canal_de_entrenamiento* y el *canal_de_validación*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/train/\".format(bucket,prefix,train_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}/validate/\".format(bucket,prefix,validate_file),\n",
    "    content_type='text/csv')\n",
    "\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar **ajustar** se entrenará el modelo.\n",
    "\n",
    "**Nota:** Este proceso puede tardar hasta 5 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-05-18-22-27-53-428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-05-18 22:27:53 Starting - Starting the training job....\n",
      "2023-05-18 22:28:19 Starting - Preparing the instances for training................\n",
      "2023-05-18 22:29:49 Downloading - Downloading input data.....\n",
      "2023-05-18 22:30:19 Training - Downloading the training image........\n",
      "2023-05-18 22:31:04 Training - Training image download completed. Training in progress.....\n",
      "2023-05-18 22:31:30 Uploading - Uploading generated training model..\n",
      "2023-05-18 22:31:41 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de completar el entrenamiento, estará listo para probar y evaluar el modelo. Sin embargo, hará pruebas y validación en laboratorios posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¡Felicitaciones!\n",
    "\n",
    "Ha completado este laboratorio y ahora puede terminarlo siguiendo las instrucciones de la guía del laboratorio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
